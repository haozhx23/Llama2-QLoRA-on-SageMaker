{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8f9b81b-5f71-40dc-8e66-a2dbcfb8a733",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model deployment with HuggingFace Accelerate engine integrated in LMI (Large Moder Inference Container) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b85752-bc9d-468b-8b2c-52def72d0bd7",
   "metadata": {},
   "source": [
    "We use DJLServing as the model serving solution in this example that is bundled in the LMI container. DJLServing is a high-performance universal model serving solution powered by the Deep Java Library (DJL) that is programming language agnostic. To learn more about DJL and DJLServing, you can refer to our recent blog post (https://aws.amazon.com/blogs/machine-learning/deploy-bloom-176b-and-opt-30b-on-amazon-sagemaker-with-large-model-inference-deep-learning-containers-and-deepspeed/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2185d7d-c868-4a60-89e4-d37309efbf31",
   "metadata": {},
   "source": [
    "Important to refer:\n",
    "\n",
    "LMI Samples - https://github.com/aws/amazon-sagemaker-examples/tree/main/inference/generativeai/llm-workshop\n",
    "</br>\n",
    "LMI in SageMaker Developer guide - https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-tutorials.html\n",
    "</br>\n",
    "DJLModel - https://sagemaker.readthedocs.io/en/stable/frameworks/djl/index.html\n",
    "</br>\n",
    "DJL Serving 可配置参数列表 - https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-configuration.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320c336f-7651-4807-beac-fa09f3682b6a",
   "metadata": {},
   "source": [
    "#### Init SageMaker Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de8413b-160c-4604-8953-f83ac27e5f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "default_bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef7e630-1706-46a7-a513-ac8625008a38",
   "metadata": {},
   "source": [
    "#### Construct artifacts and deploy on SageMaker endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df682fa-8be3-4893-a40f-cc0bcb5680a0",
   "metadata": {},
   "source": [
    "<mark>**NOTE**\n",
    ": Copy the S3 path where the Training Job saves the model artifacts to. And give it to the option.s3url entry.</mark>\n",
    "\n",
    "\n",
    "In this sample notebook, we use <mark>Python (HuggingFace Accelerate)</mark> engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9799c5c5-c739-477d-aff5-d51cb69a01b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile serving.properties\n",
    "engine=Python\n",
    "#engine=DeepSpeed\n",
    "option.tensor_parallel_degree=1\n",
    "#option.model_id=TheBloke/Wizard-Vicuna-7B-Uncensored-HF\n",
    "option.s3url=s3://COPY_FROM_TRAINING_SCRIPT/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fef8cb9-eab4-4d22-921b-586a5d4eb727",
   "metadata": {},
   "source": [
    "Here is a list of settings that we use in this configuration file -\n",
    "\n",
    "- engine: The engine for DJL to use. In this case, we have set it to MPI.\n",
    "option.model_id: The model id of a pretrained model hosted inside a model repository on huggingface.co (https://huggingface.co/models) or S3 path to the model artefacts. \n",
    "- option.tensor_parallel_degree: Set to the number of GPU devices over which Accelerate needs to partition the model. This parameter also controls the no of workers per model which will be started up when DJL serving runs. As an example if we have a 4 GPU machine and we are creating 4 partitions then we will have 1 worker per model to serve the requests.\n",
    "\n",
    "For more details on the configuration options and an exhaustive list, you can refer the documentation - https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-configuration.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ffb223-0ebf-4c0f-b989-c4b0b26b4c47",
   "metadata": {},
   "source": [
    "#### Tar the model serving code and upload to S3 (for SageMaker Endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a3c862-a19e-4755-821b-00702c887f30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Construct code artifacts tar\n",
    "code_tarname = 'llama2-qlora-merged-acc'\n",
    "\n",
    "!mkdir -p {code_tarname}\n",
    "!rm -rf {code_tarname}.tar.gz\n",
    "!rm -rf {code_tarname}/.ipynb_checkpoints\n",
    "\n",
    "!mv model.py {code_tarname}/\n",
    "!mv requirements.txt {code_tarname}/\n",
    "!mv serving.properties {code_tarname}/\n",
    "!tar czvf {code_tarname}.tar.gz {code_tarname}/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6aebd0-3260-45e1-92a6-ca4b7163490c",
   "metadata": {},
   "source": [
    "Upload the tar of CODE to 'any' valid S3 path (different from hf model artifacts path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26da9046-26de-4cbf-a3ce-c01769715690",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_code_artifact = sess.upload_data(f\"{code_tarname}.tar.gz\", \n",
    "                                    default_bucket, \n",
    "                                    sagemaker.utils.name_from_base(\"tmp/v0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d5174d-d3c5-4ee1-b440-3ab4a1ece4a1",
   "metadata": {},
   "source": [
    "Use a proper LMI version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955fce1c-e119-4e4f-ae57-c50568d04e94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify a inference container version, \n",
    "# - https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers\n",
    "inference_image_uri = f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.23.0-deepspeed0.9.5-cu118\"\n",
    "\n",
    "# name a SageMaker Endpoint\n",
    "endpoint_name = sagemaker.utils.name_from_base(code_tarname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320bd4be-2397-45c5-bca1-c9269379bd5d",
   "metadata": {},
   "source": [
    "Register/Declare the model with proper configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b733c70d-272a-4bf0-8b80-7f05222efef6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "model = Model(image_uri=inference_image_uri,\n",
    "              model_data=s3_code_artifact, \n",
    "              role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb9afcf-7747-48d1-99af-26edcf39d71b",
   "metadata": {},
   "source": [
    "Trigger the model deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7ca6ba-9ab2-4f1f-b2a6-519e2a42319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.deploy(initial_instance_count = 1,\n",
    "             instance_type = 'ml.g4dn.xlarge', \n",
    "             endpoint_name = endpoint_name,\n",
    "             container_startup_health_check_timeout = 900\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4152e2c8-53fd-4429-9484-e51e1e29a25e",
   "metadata": {},
   "source": [
    "#### Init predictor and invoke specified endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca14375d-a1af-43d8-988a-7ecd5d5bdf40",
   "metadata": {},
   "source": [
    "If you only need to invoke a in-service Endpoint without the need of deployment, just start from this step and pass the SageMaker Endpoint name to the Predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dfa03d-318e-4a4a-8892-c0a250765d44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import serializers, deserializers\n",
    "\n",
    "# Or copy endpoint name from SageMaker console for direct invocation\n",
    "# endpoint_name = 'llama2-merge-model-2023-08-19-04-42-02-574'\n",
    "\n",
    "predictor = sagemaker.Predictor(\n",
    "            endpoint_name=endpoint_name,\n",
    "            sagemaker_session=sess,\n",
    "            serializer=serializers.JSONSerializer(),\n",
    "            deserializer=deserializers.JSONDeserializer(),\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9090826e-1e7e-46ab-88bf-b15ed2d20bae",
   "metadata": {},
   "source": [
    "The payload pattern should align with that defined in the code (model.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c691bb4d-124c-4f5a-a22c-dbcfae44b4a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.predict(\n",
    "    {\"inputs\": [\"tuna sandwich nutritional content is \", \"I need to cook a good pizza, so \"], \n",
    "     \"parameters\": {\"max_new_tokens\": 200}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4d0c3e-1d1e-42a9-a1fb-8a186cc0f840",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n3 -r1\n",
    "predictor.predict(\n",
    "    {\"inputs\": \"tuna sandwich nutritional content is \", \n",
    "     \"parameters\": {\"max_new_tokens\": 200}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4ef1f7-8721-4929-9b1e-52d51da3d80a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
